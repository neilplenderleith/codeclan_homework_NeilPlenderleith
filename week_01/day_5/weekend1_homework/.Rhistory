mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(avg_time_per_author = mean((time_between_books), na.rm = T),
books_written = sum(n())) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall)
books_written = n() %>%
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written > 1) %>%
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(avg_time_per_author = mean((time_between_books), na.rm = T),
books_written = n()) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall)
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written > 1) %>%
mutate(time_between_books = pub_date - last_book_pub_date)
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written > 1) %>%
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall)
book_publish_speed
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written > 1) %>%
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall) %>%
arrange(avg_time_per_author)
book_publish_speed
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written > 1) %>%
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall) %>%
arrange(desc(avg_time_per_author))
book_publish_speed
19724/365
# lets find the slowest authors
book_publish_speed %>%
slice_max(avg_time_per_author, n = 10)
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written >= 3) %>% # filter for >3 books written
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall) %>%
arrange(desc(avg_time_per_author))
# lets find the slowest authors
book_publish_speed %>%
slice_max(avg_time_per_author, n = 10)
# lets find the fastest authors
book_publish_speed %>%
slice_min(avg_time_per_author, n = 10)
# lets find the slowest authors
book_publish_speed %>%
slice_max(avg_time_per_author, n = 10)
books_clean %>%
group_by(isbn) %>%
summarise(count = n()) %>%
filter(count > 1)
books_clean %>%
group_by(isbn) %>%
summarise(count = n())
books_clean %>%
group_by(isbn13) %>%
summarise(count = n()) %>%
filter(count > 1)
books_clean %>%
group_by(title) %>%
summarise(count = n()) %>%
filter(count > 1)
books_clean %>%
group_by(title) %>%
summarise(count = n()) %>%
filter(count > 1) %>%
arrange(desc(count))
books_clean %>%
slice_max(average_rating, n = 50)
books_clean %>%
filter(ratings_count > 10, num_pages > 10) %>%
slice_max(average_rating, n = 50)
books_clean %>%
slice_max(average_rating, n = 50)
# well the result of that includes books with 1 rating, books with 2 pages etc
# lets tidy it up a bit and filter for >10 pages and ratings
books_clean %>%
filter(ratings_count > 10, num_pages > 10) %>%
slice_max(average_rating, n = 50) %>%
select(title)
books_clean %>%
filter(sci_fi_authors %in% authors))
books_clean %>%
filter(sci_fi_authors %in% authors)
sci_fi_authors <- c("Jules Verne", "H. G. Wells", "Aldous Huxley",
"Robert Heinlein", "Arthur C. Clarke", "Frank Herbert",
"Isaac Asimov", "Ray Bradbury", "William Gibson",
"Orson Scott Card")
books_clean %>%
filter(sci_fi_authors %in% authors)
books_clean %>%
filter(authors %in% sci_fi_authors)
#lets add in 10 famous sci-fi authors
sci_fi_authors <- c("Jules Verne", "H. G. Wells", "Aldous Huxley",
"Robert Heinlein", "Arthur C. Clarke", "Frank Herbert",
"Isaac Asimov", "Ray Bradbury", "William Gibson",
"Orson Scott Card")
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating, n = 10) %>%
select(title, average_rating)
#lets add in 10 famous sci-fi authors
sci_fi_authors <- c("Jules Verne", "H. G. Wells", "Aldous Huxley",
"Robert Heinlein", "Arthur C. Clarke", "Frank Herbert",
"Isaac Asimov", "Ray Bradbury", "William Gibson",
"Orson Scott Card")
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating, n = 10) %>%
#select(title, aaverage_rating)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating, n = 10)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating, n = 20)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
slice_max(average_rating)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
arrange(average_rating)
books_clean %>%
filter(authors %in% sci_fi_authors) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, title = unique(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, unique(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, title %in% unique(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, title = unique(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, title == unique(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
filter(authors %in% sci_fi_authors, distinct(title))) %>%
books_clean %>%
filter(authors %in% sci_fi_authors, distinct(title)) %>%
arrange(desc(average_rating) )
books_clean %>%
distinct(title) %>%
filter(authors %in% sci_fi_authors) %>%
arrange(desc(average_rating) )
books_clean %>%
distinct(title)
books_clean %>%
distinct(title, .keep_all = T) %>%
filter(authors %in% sci_fi_authors) %>%
arrange(desc(average_rating) )
library(tidyverse)
library(skimr)
library(janitor)
library(lubridate)
library(anytime)
library(ggplot2)
books <- read.csv("books.csv")
#I used these 3 functions to get an initial idea of the dataset
# glimpse(books)
# summary(books)
# skim(books)
# Lets do a count of N/A and missing values etc
books %>%
summarise(across(.fns = ~ sum(is.na(.x))))
# this counts N/A values across all columns
books %>%
summarise(across(.fns = ~ sum(is_empty(.x))))
# this counts empty values in columns
books %>%
summarise(across(.fns = ~ sum(.x == "")))
# This counts empty strings across all columns
books %>%
summarise(across(.fns = ~ sum(.x == 0,
na.rm = T)))
#This counts any 0 values across all columns
# here we drop NA's, clean out a couple of random rows
# change numpages to integer from character
# change average rating to numeric
# impute into num_pages the mean of num_pages column for 0 values
# i came back to add pub_date which is a <date> column
# also added year_written pulled from above pub_date value
books_clean <- books %>%
drop_na() %>% # Lets drop the 4 N/A rows
filter(!num_pages %in% c("eng", "en-US")) %>%
mutate(num_pages = as.integer(num_pages),
average_rating = as.numeric(average_rating)) %>%
mutate(num_pages = if_else(num_pages == 0,
as.integer(mean(num_pages)),
num_pages)) %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date))
books_clean %>%  # quick check to see "0"s have gone (changed to mean)
summarise(across(.cols = num_pages, .fns = ~ sum(.x == 0,
na.rm = T)))
# OK we can live with this data for the moment lets start somes tasks
books_clean %>%
group_by(authors) %>%
summarise(books_authored = n()) %>%
slice_max(books_authored, n = 10)
# Turns out it returns a top-13 as the 10th place is tied 4 ways
# We will allow this but could change it with the "with_ties" argument
top_10_reviewed_authors <- books_clean %>%
group_by(authors) %>%
summarise(books_authored = n(),
author_average_reviews = round(mean(average_rating), 2)) %>%
filter(books_authored > 3) %>%
slice_max(author_average_reviews, n = 10) %>%
select(authors, author_average_reviews)
top_10_reviewed_authors
languages_percent <- books_clean %>%
count(language_code) %>%
arrange(desc(n)) %>%
mutate(percentage_language = round((n/sum(n))*100, 2)) %>%
slice_max(percentage_language, n = 10) %>%
select(language_code, percentage_language)
languages_percent
books_date <- books_clean %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date)) %>%
group_by(year_written) %>%
summarise(rating_per_year = round(mean(average_rating), 2), no_ratings_per_year = n()) %>%
filter(no_ratings_per_year >= 4) %>%
slice_max(rating_per_year, n = 10)
books_date
# I had to have a wee look at this on a v simple graph
# ungroup()
#  ggplot(books_date, aes(x = year_written, y = rating_per_year)) +
#   geom_point() +
#   geom_smooth()
# Summarise by authors containing a "/" symbol (these should be multi-authored)
# then count each side and do some tidying up
book_solo_yn <- books_clean %>%
summarise(multi_author = grepl("/", books_clean$authors, fixed = T)) %>%
count(multi_author) %>%
mutate(multi_authored = if_else(multi_author == F,
"solo",
"multiple authors"),
.keep = "unused")
book_solo_yn
avg_rating_solo <- books_clean %>%
mutate(solo_author = grepl("/", books_clean$authors, fixed = T)) %>%
filter(solo_author == F) %>%
summarise(avg_rating_solo = mean(average_rating), count = n())
avg_rating_solo
avg_rating_multi <- books_clean %>%
mutate(solo_author = grepl("/", books_clean$authors, fixed = T)) %>%
filter(solo_author == T) %>%
summarise(avg_rating_multi = mean(average_rating), count = n())
avg_rating_multi
# 6561 solo written books, 4562 multiple authored books
# multi authored books win 3.96 to 3.91 though theres not much in it
# counts books by publisher, assigns an average review for each publisher
# then filters for books > 3 and arranges by number of books or avg review
top_publishers <- books_clean %>%
group_by(publisher) %>%
summarise(books_by_publisher = n(),
publisher_average_reviews = round(mean(average_rating), 2)) %>%
filter(books_by_publisher > 3) %>%
#arrange(desc(publisher_average_reviews))
arrange(desc(books_by_publisher))
top_publishers
books_clean %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date)) %>%
filter(year_written >= 1990 &
year_written <2000 &
publisher == "Vintage") %>%
select(title, text_reviews_count) %>%
slice_max(text_reviews_count, n = 20)
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written >= 3) %>% # filter for >3 books written
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall) %>%
arrange(desc(avg_time_per_author))
# lets find the slowest authors
book_publish_speed %>%
slice_max(avg_time_per_author, n = 10)
# lets find the fastest authors
book_publish_speed %>%
slice_min(avg_time_per_author, n = 10)
# Although I think the general principle is sound it appears the data is not
# quite clean enough for this operation. Nor does my theory take into accounts
# republications of the same works years afterword or indeed multi language copies.
books_clean %>%
group_by(title) %>%
summarise(count = n()) %>%
filter(count > 1) %>%
arrange(desc(count))
# There are 487 books which appear to have multiple copies.
# The record is 9 copies with "The Brothers Karamazov" and "The Iliad"
# This would be republished books as well as multi-language copies
#lets add in 10 famous sci-fi authors
sci_fi_authors <- c("Jules Verne", "H. G. Wells", "Aldous Huxley",
"Robert Heinlein", "Arthur C. Clarke", "Frank Herbert",
"Isaac Asimov", "Ray Bradbury", "William Gibson",
"Orson Scott Card")
books_clean %>%
distinct(title, .keep_all = T) %>% # filter out duplicate titles
filter(authors %in% sci_fi_authors) %>%
arrange(desc(average_rating) )
library(tidyverse)
library(skimr)
library(janitor)
library(lubridate)
library(anytime)
library(ggplot2)
books <- read.csv("books.csv")
#I used these 3 functions to get an initial idea of the dataset
# glimpse(books)
# summary(books)
# skim(books)
# Lets do a count of N/A and missing values etc
books %>%
summarise(across(.fns = ~ sum(is.na(.x))))
# this counts N/A values across all columns
books %>%
summarise(across(.fns = ~ sum(is_empty(.x))))
# this counts empty values in columns
books %>%
summarise(across(.fns = ~ sum(.x == "")))
# This counts empty strings across all columns
books %>%
summarise(across(.fns = ~ sum(.x == 0,
na.rm = T)))
#This counts any 0 values across all columns
# here we drop NA's, clean out a couple of random rows
# change numpages to integer from character
# change average rating to numeric
# impute into num_pages the mean of num_pages column for 0 values
# i came back to add pub_date which is a <date> column
# also added year_written pulled from above pub_date value
books_clean <- books %>%
drop_na() %>% # Lets drop the 4 N/A rows
filter(!num_pages %in% c("eng", "en-US")) %>%
mutate(num_pages = as.integer(num_pages),
average_rating = as.numeric(average_rating)) %>%
mutate(num_pages = if_else(num_pages == 0,
as.integer(mean(num_pages)),
num_pages)) %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date))
books_clean %>%  # quick check to see "0"s have gone (changed to mean)
summarise(across(.cols = num_pages, .fns = ~ sum(.x == 0,
na.rm = T)))
# OK we can live with this data for the moment lets start somes tasks
books_clean %>%
group_by(authors) %>%
summarise(books_authored = n()) %>%
slice_max(books_authored, n = 10)
# Turns out it returns a top-13 as the 10th place is tied 4 ways
# We will allow this but could change it with the "with_ties" argument
top_10_reviewed_authors <- books_clean %>%
group_by(authors) %>%
summarise(books_authored = n(),
author_average_reviews = round(mean(average_rating), 2)) %>%
filter(books_authored > 3) %>%
slice_max(author_average_reviews, n = 10) %>%
select(authors, author_average_reviews)
top_10_reviewed_authors
languages_percent <- books_clean %>%
count(language_code) %>%
arrange(desc(n)) %>%
mutate(percentage_language = round((n/sum(n))*100, 2)) %>%
slice_max(percentage_language, n = 10) %>%
select(language_code, percentage_language)
languages_percent
books_date <- books_clean %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date)) %>%
group_by(year_written) %>%
summarise(rating_per_year = round(mean(average_rating), 2), no_ratings_per_year = n()) %>%
filter(no_ratings_per_year >= 4) %>%
slice_max(rating_per_year, n = 10)
books_date
# I had to have a wee look at this on a v simple graph
# ungroup()
#  ggplot(books_date, aes(x = year_written, y = rating_per_year)) +
#   geom_point() +
#   geom_smooth()
# Summarise by authors containing a "/" symbol (these should be multi-authored)
# then count each side and do some tidying up
book_solo_yn <- books_clean %>%
summarise(multi_author = grepl("/", books_clean$authors, fixed = T)) %>%
count(multi_author) %>%
mutate(multi_authored = if_else(multi_author == F,
"solo",
"multiple authors"),
.keep = "unused")
book_solo_yn
avg_rating_solo <- books_clean %>%
mutate(solo_author = grepl("/", books_clean$authors, fixed = T)) %>%
filter(solo_author == F) %>%
summarise(avg_rating_solo = mean(average_rating), count = n())
avg_rating_solo
avg_rating_multi <- books_clean %>%
mutate(solo_author = grepl("/", books_clean$authors, fixed = T)) %>%
filter(solo_author == T) %>%
summarise(avg_rating_multi = mean(average_rating), count = n())
avg_rating_multi
# 6561 solo written books, 4562 multiple authored books
# multi authored books win 3.96 to 3.91 though theres not much in it
# counts books by publisher, assigns an average review for each publisher
# then filters for books > 3 and arranges by number of books or avg review
top_publishers <- books_clean %>%
group_by(publisher) %>%
summarise(books_by_publisher = n(),
publisher_average_reviews = round(mean(average_rating), 2)) %>%
filter(books_by_publisher > 3) %>%
#arrange(desc(publisher_average_reviews))
arrange(desc(books_by_publisher))
top_publishers
books_clean %>%
mutate(pub_date = anydate(publication_date),
year_written = year(pub_date)) %>%
filter(year_written >= 1990 &
year_written <2000 &
publisher == "Vintage") %>%
select(title, text_reviews_count) %>%
slice_max(text_reviews_count, n = 20)
book_publish_speed <- books_clean %>%
arrange(authors, pub_date) %>%
group_by(authors) %>%
mutate(last_book_pub_date = lag(pub_date),
books_written = n()) %>%
filter(books_written >= 3) %>% # filter for >3 books written
mutate(time_between_books = pub_date - last_book_pub_date) %>%
summarise(books_written = n(),
avg_time_per_author = mean((time_between_books), na.rm = T)) %>%
mutate(avg_time_overall = mean(avg_time_per_author),
difference_from_mean = avg_time_per_author - avg_time_overall) %>%
arrange(desc(avg_time_per_author))
# lets find the slowest authors
book_publish_speed %>%
slice_max(avg_time_per_author, n = 10)
# lets find the fastest authors
book_publish_speed %>%
slice_min(avg_time_per_author, n = 10)
# Although I think the general principle is sound it appears the data is not
# quite clean enough for this operation. Nor does my theory take into accounts
# republications of the same works years afterword or indeed multi language copies.
books_clean %>%
group_by(title) %>%
summarise(count = n()) %>%
filter(count > 1) %>%
arrange(desc(count))
# There are 487 books which appear to have multiple copies.
# The record is 9 copies with "The Brothers Karamazov" and "The Iliad"
# This would be republished books as well as multi-language copies
#lets add in 10 famous sci-fi authors
sci_fi_authors <- c("Jules Verne", "H. G. Wells", "Aldous Huxley",
"Robert Heinlein", "Arthur C. Clarke", "Frank Herbert",
"Isaac Asimov", "Ray Bradbury", "William Gibson",
"Orson Scott Card")
books_clean %>%
distinct(title, .keep_all = T) %>% # filter out duplicate titles
filter(authors %in% sci_fi_authors) %>%
arrange(desc(average_rating) )
